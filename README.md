
# Modified Naive Bayes Classifier for Imbalanced Text Classification 

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Documentation](https://img.shields.io/badge/docs-passing-brightgreen.svg)](./docs)

## ğŸ“š Overview

This project presents a novel approach to handling imbalanced datasets in text classification tasks by introducing a modified version of the Naive Bayes classifier. Our method incorporates class probability adjustments and feature probability modifications to better handle class imbalance issues commonly found in real-world datasets.

<img src="/api/placeholder/800/400" alt="Project Architecture Overview" />

## ğŸ” Key Features

- Custom Bayes Classifier with class imbalance handling
- Advanced probability adjustment mechanisms
- Dynamic probability evolution using differential equations
- Comprehensive comparison with traditional approaches
- Support for multi-class text classification

## ğŸ§® Mathematical Foundation

### Class Probability Adjustment

We introduce a weighted prior approach to adjust class probabilities:

```
P(y = Câ‚–) = (Nâ‚– + Î±) / (N + Î±Â·K)
```

Where:
- Nâ‚–: Number of training instances in class Câ‚–
- N: Total number of training instances
- K: Total number of classes
- Î±: Smoothing parameter

### Feature Probability Adjustment

Features are adjusted using modified Laplace smoothing:

```
P(xáµ¢|y = Câ‚–) = (Náµ¢â‚– + Î±) / (Nâ‚– + Î±Â·V)
```

Where:
- Náµ¢â‚–: Instances where feature xáµ¢ occurs in class Câ‚–
- V: Vocabulary size
- Î±: Laplace smoothing parameter

## ğŸ“Š Results

my modified approach showed improvements in handling imbalanced datasets:

- 1% improvement over traditional Naive Bayes
- Better handling of minority classes
- More stable probability estimates



## ğŸ—‚ï¸ Project Structure

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œ
â”‚   â”œâ”€â”€ data_analysis.ipynb
â”‚   â”œâ”€â”€ model_training.ipynb
â”‚   â””â”€â”€ evaluation.ipynb
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ report_part0.pdf
â”‚   â””â”€â”€ report_part1.pdf
â”‚
â”œâ”€â”€ README.md
â””
```

## ğŸ“ Implementation Details

### Custom Bayes Classifier

The implementation includes:

1. **Probability Initialization**
   - Dynamic prior probability calculation
   - Feature probability smoothing

2. **Differential Equation Integration**
   ```python
   dP(y = Câ‚–)/dt = Î²(Nâ‚–(t) + Î±)/(N(t) + Î±Â·K) - P(y = Câ‚–)
   ```

3. **Softmax Adjustment**
   ```python
   P'(xáµ¢|Câ‚–) = exp(Î±Â·P(xáµ¢|y=Câ‚–)) / Î£â‚— exp(Î±Â·P(xáµ¢|y=Câ‚—))
   ```

## ğŸ“Š Comparative Analysis

We compared our approach with:

- Traditional Naive Bayes
- LSTM-based classifiers
- BERT distillation methods



## ğŸ”§ Installation & Usage

```bash
# Clone the repository
git clone https://github.com/rarahnamoun/Multi-class-Text-Classification.git

# Run the notebooks
jupyter notebook src
```

## ğŸ“– Documentation

- [Full Technical Report](./docs/report_part0.pdf)
- [Methodology Documentation](./docs/report_part1.pdf)
